<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Bolor-Erdene Zolbayar</title>
    <link>https://bolor23erdene.github.io/posts/</link>
    <description>Recent content in Posts on Bolor-Erdene Zolbayar</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="https://bolor23erdene.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Support Vector Machine and Karush-Kuhn-Tucker</title>
      <link>https://bolor23erdene.github.io/posts/svm_kkt/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bolor23erdene.github.io/posts/svm_kkt/</guid>
      <description>This article is a brief introduction about SVM and KKT.
Table of Contents  SVM KKT  Support Vector Machine  Support Vector Machine is one of the most influential techniques in supervised machine learning (Boser et al. 1992; Cortes and Vapnik et al. 1995). It is very similar to logistic regression. The main difference is it does not output probability, but, a class identity instead. SVM predicts a positive class is present when $w^Tx+b$ is positive.</description>
    </item>
    
    <item>
      <title>Towards Deep Learning Models Resistant to Adversarial Attacks</title>
      <link>https://bolor23erdene.github.io/posts/pgd/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bolor23erdene.github.io/posts/pgd/</guid>
      <description>The goal of this paper is to train a machine learning model such that the ML system becomes resistance to adversarial examples. It is a well known fact that neural networks are vulnerable to adversarial examples. Some of the recent works explain that vulnerability of the neural networks is intrinsic. Madry et al. 2017 at MIT investigated how to create robust neural networks with robust optimization. As a result, they found a way to create successful attack and defense techniques based on robust optimization.</description>
    </item>
    
    <item>
      <title>What is GANs?</title>
      <link>https://bolor23erdene.github.io/posts/gans/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bolor23erdene.github.io/posts/gans/</guid>
      <description>The goal of one of my research is to generate adversarial network traffic that fools the network detection system. I used deep generative model GANs to create the adversarial samples. GANs are known to be best for learning finite amount of training data, interpolating the training data&#39;s distribution, and creating samples that are indistinguishable from the original training data. Basically, with GANs, I was able to generate huge amount adversarial traffic by keeping the features that are intrinsic to the attacks and changing the unimportant features so that the traffic bypasses the detection system.</description>
    </item>
    
    <item>
      <title>The Book of Why</title>
      <link>https://bolor23erdene.github.io/posts/book-of-why/</link>
      <pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://bolor23erdene.github.io/posts/book-of-why/</guid>
      <description>https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols https://lmyint.github.io/post/book-of-why/
I just finished reading The Book of Why by Judea Pearl and Dana Mackenzie, and I really enjoyed it. I had been wanting to read it for some time now because I know very little about methodology relating to causal diagrams and structure learning. The book provides an overview of the main ideas that formed and historical events that led up to what Pearl calls the &amp;ldquo;Causal Revolution&amp;rdquo;, a burgeoning of the direct interrogation of causation as opposed to its implicit renouncement in science for a period before then.</description>
    </item>
    
  </channel>
</rss>